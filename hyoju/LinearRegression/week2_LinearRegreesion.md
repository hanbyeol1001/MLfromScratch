# MachineLearning

## Linear Regression
선형 회귀 모델 : 데이터의 선형 관계를 기반으로 예측을 수행하는 모델로, 독립 변수와 종속 변수 사이의 선형관계를 설명. 

- 선형 회귀는 주어진 데이터와 예측 데이터의 오차 평균을 최소화 할 수 있는 **최적의 기울기와 절편을 찾는 것**을 목적으로 함


### 1. 단순 선형 회귀 분석
   y = wx + b 
   
   독립 변수 x와 곱해지는 값 w를 머신 러닝에서는 가중치(weight) 별도로 더해지는 b(bias)는 편향이라고 함. 직선의 방정식에서는 직선의 기울기와 절편을 의미함.


### 2. 다중 선형 회귀 분석
   y = w1x1 + w2x2 + ... + wnxn + b
   
   
   다수의 요소를 가지고 예측을 하고 싶을 때 다중 선형 회귀분석.



### 3. 선형 회귀에서 최적의 기울기와 절편을 찾는 2가지 방법 
: 정규방정식 ( Normal Equation), 경사하강법(Gradient Descent)

- 정규방정식 : 최소제곱해(Least Squares Solution)을 찾기 위한 대표적인 방법으로 대수적인 방법을 통해 최소제곱해를 구함.
    - 최소제곱해를 찾아 선형회귀 모델 추정하는 법을 Ordinary Least Squares라고 부름.
    - Scikit-Learn의 경우, 선형회귀를 할 때 OLS를 사용하고 있음.
- 경사하강법 : 손실 함수의 1차 미분계수를 이용해 함수의 최솟값을 찾아가는 방법. 손실 함숫값이 낮아지는 방향으로 독립 변수 값을 변형시켜 가면서 최종적으로 최소 함숫값을 갖도록 하는 독립 변수 값을 찾는 방법. 지역최저점(local minima)에 빠질 수 있다는 문제점이 있음.

→ 정규방정식은 평균제곱오차를 이용하여 만들어지며 행렬 연산을 통해 한 번에 최적해를 구하기 때문에 행렬의 크기가 커질수록 계산 시간이 오래 걸림. 경사하강법은 기본적으로 소요되는 시간이 길지만, 행렬의 크기가 커지더라도 일정 시간 안에 최적해를 찾음. **즉, 행렬의 크기가 크지 않다면 정규방정식, 크다면 경사하강법을 사용하는 것이 좋음.**
