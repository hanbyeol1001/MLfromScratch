# 🚀 K-Nearest Neighbors (KNN) 알고리즘
## 1. KNN이란?
> K-Nearest Neighbors(KNN) 알고리즘은 **지도 학습(Supervised Learning)** 방식의 분류(Classification) 및 회귀(Regression) 모델.  
새로운 데이터 포인트가 주어졌을 때, 가장 가까운 `K`개의 이웃을 참조하여 클래스를 예측하거나 값을 결정하는 방식이다.
---
## 2. KNN의 작동 원리
1. 새로운 데이터 포인트(예측할 데이터)가 주어진다.
2. 훈련 데이터 중에서 새로운 데이터와 가장 가까운 `K`개의 이웃을 찾는다.
3. 분류(Classification):  
    - 가장 많은 빈도로 등장하는 클래스(Label)를 새로운 데이터의 클래스로 지정한다. (다수결)
4. 회귀(Regression):  
    - `K`개의 이웃의 평균 값을 새로운 데이터의 예측 값으로 설정한다.
---
## 3. KNN의 특징
-  **간단하고 직관적인 알고리즘**  
- **비모수적(Non-parametric) 학습 방식** → 데이터의 분포를 가정하지 않는다.  
- **훈련 과정이 거의 필요 없음 (Lazy Learning)**  
- **거리 기반 계산** → 유클리드 거리(Euclidean Distance)가 가장 일반적으로 사용됨.  
- **고차원 데이터에서는 성능 저하 가능 (차원의 저주, Curse of Dimensionality)**
---
## 4. 거리 계산 방법
KNN은 데이터 간 거리를 기반으로 가까운 `K`개의 이웃을 찾는다.  
자주 사용되는 거리 측정 방법은 다음과 같다.
### (1) 유클리드 거리 (Euclidean Distance)
두 점 $ A(x_1, y_1) , B(x_2, y_2) $ 간의 거리는 다음과 같다.
$$
d(A, B) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$
### (2) 맨해튼 거리 (Manhattan Distance)
$$
d(A, B) = |x_2 - x_1| + |y_2 - y_1|
$$
### (3) 코사인 유사도 (Cosine Similarity)
벡터 간 각도를 기반으로 거리를 측정한다.
$$
\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}
$$
---
## 5. K 값의 선택
- `K` 값이 너무 작으면(예: 1) **노이즈에 민감해 과적합(Overfitting)이 발생**할 수 있음.
- `K` 값이 너무 크면(예: 전체 데이터 수에 가까움) **너무 많은 이웃을 고려하여 일반화 성능이 저하**될 수 있음.
- 일반적으로 **홀수 값**을 사용하고, **적절한 `K`를 찾기 위해 교차 검증(Cross Validation)을 활용**함.
---
## 6. KNN의 장단점
### ✅ 장점
- 단순하고 구현이 쉬움.
- 학습(Training) 과정이 거의 필요 없음 (Lazy Learning).
- 특정 데이터 분포를 가정하지 않아 유연하게 적용 가능.
### ❌ 단점
- 데이터가 많을수록 예측 시간이 오래 걸림 (메모리 사용량 증가).
- 고차원 데이터에서는 차원의 저주(Curse of Dimensionality)로 인해 성능이 저하됨.
- 이상치(Outlier)에 영향을 받을 수 있음.