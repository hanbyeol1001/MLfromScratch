
# 🔐 로지스틱 회귀 (Logistic Regression)

## 1. 개요  
로지스틱 회귀(Logistic Regression)는 **이진 분류(Binary Classification)** 문제를 해결하기 위한 지도 학습 알고리즘이다. 선형 회귀처럼 입력 변수의 선형 조합을 사용하지만, 결과를 **확률**로 해석하기 위해 시그모이드(Sigmoid) 함수를 적용하여 **출력값을 0과 1 사이로 제한**한다.

---

## 2. 로지스틱 회귀의 원리

### (1) 선형 결합  
먼저, 선형 회귀와 유사하게 입력 변수 $X$와 가중치 $W$를 선형 결합한다.

$$
z = W^T X + b
$$

여기서:
- $z$: 선형 회귀의 출력 (logit)
- $W$: 가중치 벡터
- $X$: 입력 벡터
- $b$: 편향 (bias)

### (2) 시그모이드 함수 적용  
선형 결합 결과 $z$를 **시그모이드 함수**에 통과시켜 확률 값으로 변환한다:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

예측값 $\hat{Y}$는 다음과 같이 정의된다:

$$
\hat{Y} = \sigma(W^T X + b)
$$

$\hat{Y}$는 입력이 특정 클래스(예: 1)에 속할 **확률**을 의미한다.

---

## 3. 손실 함수 (Loss Function)

로지스틱 회귀에서는 평균 제곱 오차(MSE) 대신 **이진 크로스 엔트로피(Binary Cross Entropy)** 손실 함수를 사용한다.

$$
L = -\frac{1}{n} \sum_{i=1}^{n} \left[ Y_i \log(\hat{Y}_i) + (1 - Y_i) \log(1 - \hat{Y}_i) \right]
$$

여기서:
- $Y_i$: 실제 클래스 (0 또는 1)
- $\hat{Y}_i$: 예측 확률 (0 ~ 1)
- $n$: 샘플 수

크로스 엔트로피 손실은 예측이 실제 클래스와 다를수록 큰 값을 갖는다.

---

## 4. 최적화 방법

### 경사 하강법 (Gradient Descent)  
로지스틱 회귀도 경사 하강법을 통해 파라미터 $W$와 $b$를 업데이트한다.

$$
W = W - \alpha \cdot \frac{\partial L}{\partial W}
$$

$$
b = b - \alpha \cdot \frac{\partial L}{\partial b}
$$

여기서:
- $\alpha$: 학습률 (learning rate)
- $\frac{\partial L}{\partial W}$, $\frac{\partial L}{\partial b}$: 크로스 엔트로피 손실에 대한 그래디언트

---

## 5. 로지스틱 회귀의 출력 해석

모델의 출력 $\hat{Y}$는 **확률 값**이므로, 이를 임계값(threshold)과 비교하여 **클래스 라벨**로 변환한다.

예:
- $\hat{Y} \geq 0.5$ → 클래스 1
- $\hat{Y} < 0.5$ → 클래스 0

임계값은 문제에 따라 조정할 수 있다.

---

## 6. 확장: 다중 클래스 로지스틱 회귀 (Multinomial Logistic Regression)

두 개 이상의 클래스가 있는 경우 **소프트맥스(Softmax)** 함수를 사용한 **다항 로지스틱 회귀(Multinomial Logistic Regression)**를 적용한다.

$$
P(y = k \mid x) = \frac{e^{W_k^T x + b_k}}{\sum_{j=1}^{K} e^{W_j^T x + b_j}}
$$

여기서:
- $K$: 클래스의 수
- $W_k$: 클래스 $k$에 대한 가중치 벡터

---

## 7. 로지스틱 회귀의 가정

1. **독립성(Independence)**: 각 샘플은 독립적이다.  
2. **선형성(Linearity of log-odds)**: 입력 변수와 log-odds(로그 확률 비율) 사이에 선형 관계가 있어야 한다.  
3. **다중 공선성 없음(Multicollinearity)**: 독립 변수들 간의 강한 상관관계가 없어야 한다.

---

## 8. 로지스틱 회귀의 장단점

### 장점
- 출력값이 확률로 해석 가능
- 계산이 효율적이고 빠름
- 해석이 직관적 (특히 피처 중요도 해석이 쉬움)
- 이상치에 비교적 덜 민감함 (비선형 함수 사용)

### 단점
- 선형 결정 경계를 전제로 하기 때문에 복잡한 문제에 한계
- 고차원/복잡한 데이터에서는 성능이 제한될 수 있음
- 다중 공선성 존재 시 해석이 어려움
